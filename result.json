{
  "Title": "Automated Program Repair in the Era of Large Pre-trained Language Models",
  "Outline": [
    {
      "Title": "Abstract",
      "Outline": "研究大纲：\n- 当前传统的和基于学习的自动程序修复（APR）技术的补丁种类有限，难以修复复杂的错误。\n- 使用数十亿个文本/代码令牌训练的大型预训练语言模型（LLMs）潜在地可以解决这个问题。\n- 现有的使用LLMs进行APR的工作要么缺乏最先进的模型，要么缺乏实际的评估。\n- 本研究对9个最新的LLMs进行了广泛的评估，使用了3种不同的修复设置和5个数据集，涵盖多种语言。\n- 研究表明，直接应用最先进的LLMs在所有数据集上都优于现有的APR技术。\n- 较大的LLMs倾向于实现更好的性能。\n- 在错误行之后包括后缀代码对于生成更多的修复和具有更高编译率的补丁很重要。\n- LLMs可用于有效的补丁排序和补丁正确性检查。\n- 增加样本大小和结合补丁模板信息可以进一步改进基于LLM的APR。\n"
    },
    {
      "Title": "INTRODUCTION",
      "Outline": "- 软件错误的普遍性和自动程序修复（APR）工具的需求\n- 传统APR技术和修复模板的限制\n- 基于学习的APR工具将程序修复建模为神经机器翻译（NMT）问题\n- 基于学习的APR工具由于训练数据限制的局限性\n- 大型预训练语言模型（LLMs）的介绍及其在程序修复中的潜力，而无需依赖历史错误修复\n- AlphaRepair作为使用LLMs进行程序修复的示例\n- 评估LLMs的代码相关任务以及需要准确的评估指标\n- 作者对修复真实项目的LLMs进行评估的工作和使用的不同实验设置\n- 本文在弥合LLMs和APR之间的差距、进行广泛评估和提供实用指南方面的贡献\n"
    },
    {
      "Title": "BACKGROUND AND RELATED WORK",
      "Outline": "- 该段讨论了在生成式AI领域使用Transformer等大型语言模型（LLM）所面临的挑战和限制。\n- 一个挑战是对生成的输出缺乏细粒度控制，因为LLM倾向于生成连贯但不一定准确或合适的文本。\n- 另一个挑战是使用LLM的伦理问题，因为它们可以轻易地用于生成恶意或有害内容。\n- 当前LLM的局限性包括对大量数据的依赖，这些数据可能存在偏见并延续现有的不平等。\n- 该段落以强调需要研究来解决这些挑战，并开发改进LLM的可控性和道德使用的技术。\n- Large Pre-Trained Language Model\n - 大语言模型（LLMs）在自然语言处理任务中取得了令人印象深刻的性能。\n - LLMs遵循Transformer架构。\n - LLMs首先进行自我训练，然后进行下游任务的微调。\n - 通过提示工程，LLMs可以在没有微调的情况下执行下游任务。\n - LLMs可以分为仅编码器、仅解码器和编码器-解码器模型。\n - 仅编码器模型使用MLM目标学习数据表示。\n - 仅解码器模型是大型生成模型，根据前面的所有标记（左上下文或前缀）预测下一个标记输出。\n - 编码器-解码器模型被用于序列到序列任务。\n - 跨度预测任务和将MLM与生成模型相结合也被用于LLMs。\n - 我们选择了9个最先进的LLMs进行研究。\n- Automated Program Repair\n - 传统的APR工具分为基于启发式、基于约束和基于模板的工具。\n - 模板化的APR工具性能较好，但只能修复特定类型的bug。\n - 学习型的APR工具使用NMT技术，并需要bug修复数据。\n - AlphaRepair在零样本设置下使用CodeBERT进行APR。\n - 最近的研究在APR中应用了LLM，结果显示具有竞争力。\n - 本文对应用最先进的LLM进行多样化修复数据集的填充式和生成式APR进行了广泛研究。\n"
    },
    {
      "Title": "APPROACH",
      "Outline": "- 介绍所选择的LLMs进行评估\n- 介绍3种不同的APR生成设置\n- 设置目的\n- 强调LLM类型的优势和差异\n- 使用熵进行补丁排序策略\n- Models\n - 描述用于评估的不同LLMs\n - 选择LLMs的过程\n - 使用9个不同的LLMs进行实验\n - 所选LLMs的概述\n - 生成式模型:\n   - GPT-Neo, GPT-J, GPT-NeoX\n   - 参数和训练数据集\n - Codex\n   - 参数和训练过程\n - 填充模型:\n   - CodeT5\n   - INCODER\n - Codex用于代码填充\n- LLM-based Patch Generation\n 研究中设计了三种APR设置：完整函数生成、正确代码填充和单行生成。完整函数生成的初始输入是原始错误函数，使用特定提示进行少样本学习，使LLMs能够生成修复后的函数。正确代码填充的输入是删除错误代码块后的前缀和后缀，使用填充模型生成缺失代码。单行生成中，提供了错误位置并只需进行单行更改，可以使用填充模型或生成模型生成替换行，使用生成模型时在生成一行后停止，并且不能提供后缀代码给生成模型。\n- Patch Ranking and Validation\n - 所有三个修复任务的补丁生成过程是相似的，使用LLMs和采样来为每个bug生成多个补丁。\n - 使用核心采样（nucleus sampling）和采样温度，较低的温度会生成更相似的样本，较高的温度会生成更独特和有趣的样本。\n - 如何选择最佳温度值对于APR这样的问题并不明显，对于较容易的bug，可能更倾向于使用较低的温度值，以便快速得到一个合理的补丁。对于较难的bug，较高的温度值可以用来生成更多独特的补丁，以尝试提供一个修复。\n - 记录每个补丁的熵值，衡量生成的样本在模型中的自然程度。\n - 平均熵和总熵用于补丁排名，优先选择熵较低的补丁。\n - 之前的工作使用了平均熵或总熵，但本工作对两者进行了比较，并将其应用于补丁的正确性检查。\n - 过滤掉具有语法或语义错误的补丁，并对其余的补丁进行测试套件验证。\n"
    },
    {
      "Title": "EXPERIMENTAL SETUP",
      "Outline": "- 本段论述了阅读学术论文的重要性以及专家在理解和总结学术论文中的作用。\n- 强调了非专家在理解学术论文方面面临的挑战以及拥有能够提取关键要点并提供简洁摘要的专家的好处。\n- 该段还提到了当前的任务，即阅读和理解给定的学术论文段落并提供摘要。\n- 输出应包括该段的概述和翻译。\n- Research Questions\n 我们研究以下研究问题：\n RQ1：不同类型的大语言模型在不同的APR设置中表现如何？\n 我们研究不同大语言模型在不同的修复数据集上的有效性，跨不同的语言和不同的APR任务。此外，我们评估大语言模型在增加模型大小时的规模行为，以综合评估每个大语言模型的APR能力、计算时间和编译速度。\n RQ2：直接应用大语言模型进行APR与最先进的APR工具相比如何？\n 我们使用大语言模型与最先进的基线进行比较。我们研究大语言模型修复的独特错误，并突出直接应用大语言模型进行APR的优势。\n RQ3：大语言模型能否直接用于补丁排序和正确性检查？\n 我们使用大语言模型的内置自然度度量指标（熵）来评估大语言模型是否认为已修复的函数比有错误的函数更自然，并且熵是否可以直接用于补丁排序和正确性检查。\n RQ4：我们能进一步提高大语言模型的性能吗？\n 我们探索两个方向来进一步提高大语言模型在APR中的性能：1）增加样本数量；2）将大语言模型与模板结合使用。\n- Implementation\n 我们使用Python和PyTorch实现了生成流水线。我们使用Hugging Face加载模型权重并生成输出。对于Codex，我们使用OpenAI提供的API访问来查询模型。为了用Codex进行正确的代码填充，我们在API请求中附加了一个额外的后缀参数。我们直接重用了每个模型的权重。我们默认的生成设置使用了nucleus采样，参数为top p = 0.95，temperature = 0.8，每个bug生成200个样本。这个生成设置与之前关于LLM的研究一致。我们在一台32核的工作站上生成补丁，配备Ryzen Threadripper PRO 3975WX CPU，256 GB RAM和NVIDIA RTX A6000 GPU，运行Ubuntu 20.04.4 LTS。\n- Subject Systems\n - 评估了跨 3 种编程语言的 5 个 APR 基准\n - 关注修复在单个函数中的错误\n - 过滤基准以适应设计的修复设置\n - 每个修复数据集的详细信息如下：\n   - Defects4J 1.2 和 2.0：来自开源 Java 项目的错误，1.2 版中有 391 个错误，2.0 版中有 438 个新错误，每个错误都有开发者测试\n   - QuixBugs-Python 和 -Java：多语言修复基准，包含 40 个经典编程问题，错误来自编程挑战，有 Python 和 Java 两个版本，每个错误都附带多个测试输入和预期输出\n   - ManyBugs：C 修复数据集，来自 9 个开源项目的错误，总共有 185 个错误，每个错误都经过手动验证并分类为错误类型，评估中使用了 91 个错误\n- Compared Techniques\n - 与最先进的基于学习和传统的APR工具进行比较\n - 评估8个最近的基于学习的APR工具和12个传统的APR工具\n - 基于NMT模型的基于学习的APR基线，AlphaRepair在零-shot设置下结合LLM和模板生成补丁\n - 在完美的故障定位上与基准结果进行比较\n - 使用先前研究中的正确补丁结果\n- Evaluation Metrics\n - 评估修复性能的指标：\n   - 合理的补丁：通过所有测试用例\n   - 正确的补丁：在语法或语义上等同于参考补丁\n - 确定正确的补丁：\n   - 手动检查每个合理的补丁是否语义上等同\n - 遵循 APR 研究的标准做法\n"
    },
    {
      "Title": "RESULT",
      "Outline": "- 该段落讨论了研究人员在开发Transformer等大型语言模型(LLMs)时面临的挑战。\n- 第一个挑战是缺乏足够的训练数据来对LLMs进行细化调整以适应特定任务。\n- 第二个挑战是LLMs倾向于生成不正确或无意义的响应，这是由于它们依赖于训练数据中的统计模式。\n- 第三个挑战是难以控制LLMs的输出，以确保它们生成可靠和准确的信息。\n- 该段落的结论是解决这些挑战对于LLMs在实际应用中的成功部署至关重要。\n- RQ1: Comparison of Different LLMs\n 大语言模型（LLM）在修复效果方面的比较：\n - LLMs生成补丁的比较\n - 修复效果的扩展效应\n - 随着模型大小的增加，性能提高\n - Codex优于GPT-NeoX\n - 代码填充和单行生成可生成更多正确的修复\n - Codex在代码填充方面的表现优于函数生成\n- RQ2: Comparison against State-of-the-art APR tools\n - 比较使用LLMs进行修复与传统和基于学习的APR工具在Defects4J 1.2上的结果。\n - 结果表明，一些LLMs的性能与最先进的APR工具相当。\n - LLMs生成了大量正确的修复，同时使用较少的每个bug样本与先前方法相比。\n - 将所有LLMs生成的补丁合并在一起会产生最多的正确修复。\n - LLMs生成了其他工具尚未修复的独特的bug修复。\n - LLMs可以生成多行代码以提供正确的修复。\n - LLMs的性能超越了TBar，一个基于模板的APR工具，并且与最近的Recoder技术相竞争。\n - LLMs可以直接用于多语言修复。\n- RQ3: Patch Ranking and Correctness Checking Analysis\n - 熵计算: 可以计算每个patch的熵来衡量生成样本的自然程度。\n   - 正确和合理的patch的平均熵值低于不合理的patch。\n   - 正确的patch被认为比其他patch更自然。\n   - 正确的patch通常比合理的patch更不熵。\n - 通过LLMs进行熵计算可以帮助区分正确的patch和合理的patch。\n - Patch排序: 可以根据每个生成的patch的熵值进行排序。\n   - 首先验证熵较低的patch。\n   - 熵可以作为一个有效的衡量标准，优先验证熵较低的patch。\n   - 总熵比平均熵稍微好一些。\n   - 较短的序列通常比较长的序列具有较低的总熵，这与传统的APR或patch正确性检查技术一致。\n- RQ4: Improvements on Direct LLM-based APR\n - 使用LLM进行APR可以达到与以前的APR工具相当的性能。\n - 最佳表现模型INCODER 6.7B被选择进行进一步的实验。\n - 模型运行时间更长(每个bug 2000个样本)，并结合修复模板。\n - 在Defects4J 1.2、Defects4J 2.0单行错误和QuixBugs-Python上进行评估。\n - 结果表明，增加样本数量和使用修复模板可以改善错误修复。\n - 使用修复模板可以达到最高数量的正确修复错误。\n"
    },
    {
      "Title": "THREATS TO VALIDITY",
      "Outline": "- 内部威胁：\n  - 人工验证合理修补程序的有效性\n  - 真实开发者修补程序的潜在数据泄漏\n- 解决内部威胁的方法：\n  - 公开发布正确的修补程序和代码供公众评估\n  - 检查LLM生成的修补程序并与开发者的修补程序进行比较\n  - 检查LLM修复的错误是否包含与开发者修补不同的正确修补程序\n  - 排除单行错误以增加正确修补程序的百分比\n  - 组合LLM生成的正确修补程序以修复错误\n  - 对于某些LLM模型，检查修复的函数是否在训练数据集中\n- 外部威胁：\n  - 评估仅限于5个修复数据集和3种编程语言\n- 结果的普遍性可能因其他数据集和语言而异。\n"
    },
    {
      "Title": "DISCUSSION AND FUTURE WORK",
      "Outline": "- 本研究在软件工程（SE）领域进行了大规模的研究，探讨了应用大语言模型（LLMs）进行自动程序修复（APR）的方法。\n- 研究发现，直接应用LLMs的效果优于之前的APR技术，并且结合软件工程领域的特定技术可以进一步提高LLMs的性能。\n- 未来的研究方向有两个：\n  - 提升APR中LLMs的性能，可以利用项目特定的知识和修复特定的知识来改进LLMs的性能。\n  - 探索LLMs在其他软件工程任务中的应用，比如fuzzing、测试/预测生成、变异测试和错误种子等。\n- 作者认为他们的研究结果和技术可以应用于许多涉及代码生成/变异的相关软件工程任务，展示了LLMs在软件工程中的有前景的未来。\n"
    },
    {
      "Title": "CONCLUSION",
      "Outline": "概述：\n- 对LLM进行了自动程序修复的广泛评估\n    - 使用了9个最先进的LLM和5个不同的修复数据集\n    - 设计了不同的实际修复设置来比较和对比不同LLM的修复效果\n- 阐明了增加模型大小对APR中重要因素的规模效应\n    - 修复的错误数量\n    - 修补生成的速度\n    - 编译速度\n- 将LLM与最先进的APR工具进行比较\n    - 强调使用LLM进行APR的独特修复和优点\n- 评估LLM进行修补排序和修补正确性检查的能力\n    - 优先选择正确的修补程序以加快修复速度\n- 进一步提高LLM在APR中的性能的可能性\n    - 增加样本数量\n    - 将LLM与修复模板结合使用\n- 采用LLM进行APR及其他涉及程序生成/变异的软件工程任务的未来前景可期。\n"
    },
    {
      "Title": "REFERENCES",
      "Outline": "自动程序修复领域的介绍\n自动程序修复的重要性\n自动程序修复的挑战\n自动程序修复的不同方法\n在自动程序修复中使用大型语言模型\n自动程序修复技术的评估\n自动程序修复的未来发展方向\n"
    }
  ]
}